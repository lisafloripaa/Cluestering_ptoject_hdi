# HDI_CLUSTERING_PROJECT_HAISA

  I've started this project as my final paper for my Data Science intensive course I've have taken in Awari (link, explanation).
The main goal was to do an entire project on my own for the first time so I could learn by doing all the steps needed in a data science project using machine learning algorithms. The first think I've noticed was that fiding the right data set is an important and time consuming task! Took me a while to just stop looking all over the internet for the "perfect data", my first learning was that is there no such thing as ideal and perfect data available, and, that process it self it's an entire job path. After a couple weeks I almost decide to "buy custom data" or even make my own dataset, since we were learning about Web Scrapping during this process. Funny enough, I ended up chossing one of the firsts datasest that I downloaded, I didn't want to choose that particular one because it was "too small" to actually apply the supervised machine learning algorithms that I was intending to use it. After talking to my mentor I've realize that I could do my final paper with unsupervised algorithm. 
  The Dataset that caught my eye was from The United Nations Development Programme's website, which contains Human Development Reports, so, I've downloaded the entire dataset used to produze the "Human Development Report 2020, The next frontier: Human development and the Anthropocene". My first decision was using only the 189 Countrys, excluding the data from the so called "Other Countries or Territories" ( Democratic People's Repuplic of Korea, Monaco, Nauru, San Marino, Somalia and Tuvalu); since these Countrys/Territories do not have an specific HDI rank, and, contained a big lack of information.

